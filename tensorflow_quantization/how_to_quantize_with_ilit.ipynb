{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Intel® Low Precision Optimization Tool（ilit）をインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ilit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.量子化するモデル（Resnet50 - FP32）をダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/intel-optimized-tensorflow/models/resnet50_fp32_pretrained_model.pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.推論スクリプトのダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/hiouchiy/IntelAI/master/tensorflow_quantization/infer_script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論スクリプト（infer_script.py）について。\n",
    "\n",
    "IntelAIのGithubレポジトリには、いわゆる公式ベンチマークツールがあるのですが、今回はあえてそちらを使わずに（というかより実践的な状況を想定して）独自の推論スクリプトを用意しました。公式ツールよりも実装が緩いため、若干性能が劣る点はご容赦ください。なお、バッチサイズは1で固定しています。\n",
    "\n",
    "スクリプトパラメータの説明\n",
    "- --input_graph・・・モデルファイルのパス\n",
    "- --dataset_dir・・・画像データフォルダのパス\n",
    "- --num_images・・・推論する画像枚数（この枚数を上記画像フォルダからランダムに選んで推論します。）\n",
    "- --openvino・・・OpenVINOの推論エンジン上で推論する場合はこちらにTrueをセット下さい。当然ながらモデルファイルはIRをして下さい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.FP32モデルの性能確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは、推論スクリプトの実行確認を兼ねて、量子化前のFP32のモデルの性能を確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python infer_script.py --input_graph resnet50_fp32_pretrained_model.pb --dataset_dir /imagenet/images --num_images 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "ここから量子化↓\n",
    "# 5.モデルの入力Op、および、出力Opの名称を確認\n",
    "モデル（pb）の入出力レイヤ名を[Netron](https://lutzroeder.github.io/netron/)を使って取得します。ちなみに、今回のResNet50に場合は、入力が\"input\"で、出力が\"predict\"です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.iLitにて量子化実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iLitはあくまでもプログラミングライブラリです。従って、iLitが提供するAPIを使用して量子化を実行するためのアプリケーションを開発する必要があります。ただ、それも面倒な作業なので[iLitのGithub](https://github.com/intel/lp-opt-tool)にはサンプルコードとして、iLitのAPIを使用した量子化アプリが用意されています。以下は、その量子化アプリを利用した量子化の実行コマンドです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py --input-graph ./resnet50_fp32_pretrained_model.pb --output-graph ./resnet50_int8_pretrained_model.pb --image_size 224 --input input --output predict --data-location /imagenet/tfrecord/ --config resnet50_v1.yaml --batch-size 10 --resize_method crop --tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実行中に各パラメータの意味を確認しましょう。\n",
    "\n",
    "- --input-graph ./resnet50_fp32_pretrained_model.pb ・・・入力元のFP32のモデルファイルのパス\n",
    "- --input input ・・・モデルの入力Opの名前\n",
    "- --output predict ・・・モデルの出力Opの名前\n",
    "- --output-graph ./resnet50_int8_pretrained_model.pb ・・・出力先のINT8のモデルファイルのパス\n",
    "- --data-location /imagenet/tfrecord/ ・・・量子化用画像データのパス\n",
    "- --image_size 224 ・・・画像データのサイズ\n",
    "- --resize_method crop ・・・画像前処理としてリサイズする際の方法。デフォルトがCrop。\n",
    "- --batch-size 10 ・・・バッチサイズ\n",
    "- --config resnet50_v1.yaml ・・・量子化用の設定ファイル\n",
    "- --tune ・・・iLitによるチューニングを行う"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "量子化用の設定ファイル（YAML）の中身を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat resnet50_v1.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.量子化後のTensorFlowモデルを実行して性能比較"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、量子化後のTensorFlowモデルを実行します。先ほどと同じ推論スクリプトを使用します。\n",
    "\n",
    "Intel TensorFlowをご使用いただいていれば、アプリケーションコードを変更しなくても、INT8のモデルを自動検知し、適切なCPU命令セット（Intel VNNI等）を実行します。推論処理のスピードがどの程度向上したかをご確認下さい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python infer_script.py --input_graph resnet50_int8_pretrained_model.pb --dataset_dir /imagenet/images  --num_images 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlowでの作業は以上となります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "ここからOpenVINO↓\n",
    "# 8.OpenVINOでFP32モデルをCPUに最適化（IRに変換）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここからはIntel® OpenVINO™ Toolkitを用いた量子化方法をご紹介します。\n",
    "\n",
    "といってもまずは、元のTensorFlowのモデル（FP32）をOpenVINOのIR（Intermidiate Repretation）形式に変換するところから実施しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /opt/intel/openvino/deployment_tools/model_optimizer/mo.py --input_model=./resnet50_fp32_pretrained_model.pb --input_shape=[1,224,224,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "念のため、IR(xml+bin)が生成されていることを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更に、IRをOpenVINOの推論エンジン（IE）上で実行してみます。TensorFlowの時と同じ推論スクリプトを使用します。モデルはFP32のままですが、IRに変換することでモデルの内部構造がCPUに最適化され、大きく性能が向上したことが確認できるかと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python infer_script.py --input_graph resnet50_fp32_pretrained_model.xml --dataset_dir /imagenet/images --num_images 50 --openvino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "ここからOpenVINOで量子化↓\n",
    "# 9.OpenVINOのPOTでIRを量子化\n",
    "IRの量子化はOpenVINOのPOT（Post-Training Optimization Toolkit）を使用して行います。事前にPOTの[セットアップ](https://docs.openvinotoolkit.org/latest/_README.html#install_post_training_optimization_toolkit)を完了させて下さい。\n",
    "\n",
    "その後、量子化のための各種設定を記述したConfigファイル（JSON）を準備（ダウンロード）します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/hiouchiy/IntelAI/master/tensorflow_quantization/resnet50_int8.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に今回使用するConfigファイルの中身を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat resnet50_int8.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでPOTに関して2点補足説明です。\n",
    "\n",
    "1. POTはAccuracyCheckerという既存ツールを前提としている\n",
    "\n",
    "    AccuracyCheckerはその名の通り、モデルのAccuracyを計測するためのツールです。OpenVINOのIRに変換後のモデルはもちろん、変換前の形式（TensorFlow、PyTorch、ONNXなど）であっても実行可能です。POTはこのAccuracyCheckrを拡張した機能であるため、AccuracyCheckrへの依存関係があります。したがって、上記Configファイルの前半部分は、まさにAccuracyChecker用の設定になります。\n",
    "より詳しくは[こちら](https://docs.openvinotoolkit.org/latest/_README.html)を参照ください。\n",
    "\n",
    "\n",
    "2. POTには2つの量子化のアルゴリズムが用意されている\n",
    "\n",
    "    量子化のアルゴリズムとして下記2つのいずれかを利用可能です。より詳しくは[こちら](https://docs.openvinotoolkit.org/latest/_compression_algorithms_quantization_README.html)\n",
    "    - DefaultQuantization・・・このサンプルで利用。より量子化処理の実行時間を高速化を優先。より詳しくは[こちら](https://docs.openvinotoolkit.org/latest/_compression_algorithms_quantization_default_README.html)\n",
    "    - AccuracyAwareQuantization・・・より量子化後のAccuracyを優先。時間がかかることがある。より詳しくは[こちら](https://docs.openvinotoolkit.org/latest/_compression_algorithms_quantization_accuracy_aware_README.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いて、POTを使って量子化を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pot -c resnet50_int8.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実行が成功すると、resultsというフォルダが作成されます。そして、量子化済みのIRがその中に格納されています。\n",
    "\n",
    "results/se_resnet50_DefaultQuantization/日付日時のフォルダ/optimized/**.xml\n",
    "\n",
    "ちなみに、POTコマンドではなく、[Pythonスクリプト](https://docs.openvinotoolkit.org/latest/_sample_README.html#how_to_run_the_sample)を書いて同様のことを実現可能することも可能です。より細かなカスタマイズを行いたい時などはぜひご利用ください"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.量子化後のIRを実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下記コマンドの日時の部分（2020-10-07_12-55-36）を実際のものに書き換えてから実行ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python infer_script.py --input_graph \"results/resnet50_int8_DefaultQuantization/2020-10-07_12-55-36/optimized/resnet50_int8.xml\" --dataset_dir /imagenet/images --num_images 50 --openvino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "おまけ。AccuracyChekerを使用したモデルの精度の確認方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pot -c resnet50_int8.json -e -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.それぞれの結果をグラフ化して比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "w = 0.4\n",
    "\n",
    "tf_total_time = 26\n",
    "tf_infer_time = 20\n",
    "tf_i8_total_time = 17\n",
    "tf_i8_infer_time = 11\n",
    "ov_total_time = 17\n",
    "ov_infer_time = 10\n",
    "ov_i8_total_time = 9\n",
    "ov_i8_infer_time = 3\n",
    "\n",
    "Y1 = [tf_total_time - tf_infer_time, tf_i8_total_time - tf_i8_infer_time, ov_total_time - ov_infer_time, ov_i8_total_time - ov_i8_infer_time]\n",
    "Y2 = [tf_infer_time, tf_i8_infer_time, ov_infer_time, ov_i8_infer_time]\n",
    "\n",
    "X = np.arange(len(Y1))\n",
    "\n",
    "plt.bar(X, Y1, color='gray', width=w, label='Pre/Post', align=\"center\")\n",
    "plt.bar(X, Y2, color='blue', width=w, bottom=Y1, label='Inference', align=\"center\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel(\"Spent time per one image (msec)\")\n",
    "\n",
    "plt.xticks(X, ['TensorFlow(FP32)','TensorFlow(INT8)','OpenVINO(FP32)','OpenVINO(INT8)'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------ おまけ ------\n",
    "\n",
    "左図のように、量子化後に量子化レイヤーが連続して並ぶような場合は、性能向上が期待できます。一方、右図のように量子化レイヤーとFP32レイヤーが交互に続くようなケースは、FP32→INT8、INT8→FP32の変換が都度発生し、ボトルネックとなりえるため性能向上が期待できません。解決策として、FP32の時点で量子化ツールが対応していないレイヤー（FusedBatchNormV3など）で、かつ、除去可能であれば除去してしまうことをお薦めします。\n",
    "![量子化によって性能向上が期待できるケースとできないケース](https://github.com/hiouchiy/IntelAI/raw/master/tensorflow_quantization/quantization.png \"量子化によって性能向上が期待できるケースとできないケース\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
